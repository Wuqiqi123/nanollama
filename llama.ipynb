{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute Position encoding\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{PE}(pos, 2i) &= \\sin(pos / 10000^{2i/d}) \\\\\n",
    "\\text{PE}(pos, 2i+1) &= \\cos(pos / 10000^{2i/d})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position:\n",
      " torch.Size([1024, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "max_len = 1024\n",
    "dim = 1024\n",
    "pe = torch.zeros(max_len, dim)\n",
    "position = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) * -(math.log(10000.0) / dim)))\n",
    "pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "pe[:, 1::2] = torch.cos(position.float() * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_attention:\n",
      " tensor([[[[0.3902,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "          [0.4225, 0.1181,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "          [0.7050, 0.0965, 0.8404,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "          [0.1381, 0.1485, 0.1623, 0.3332,   -inf,   -inf,   -inf,   -inf],\n",
      "          [0.1218, 0.4098, 0.5131, 0.7131, 0.3326,   -inf,   -inf,   -inf],\n",
      "          [0.6612, 0.4494, 0.9302, 0.6350, 0.5367, 0.6191,   -inf,   -inf],\n",
      "          [0.5904, 0.4040, 0.6199, 0.9940, 0.5092, 0.5917, 0.3640,   -inf],\n",
      "          [0.8878, 0.8599, 0.9144, 0.0362, 0.6412, 0.1253, 0.4594, 0.4681]]]])\n",
      "softmax_attention:\n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5755, 0.4245, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3719, 0.2023, 0.4258, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2353, 0.2377, 0.2410, 0.2860, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1459, 0.1946, 0.2158, 0.2636, 0.1802, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1685, 0.1364, 0.2206, 0.1642, 0.1488, 0.1616, 0.0000, 0.0000],\n",
      "          [0.1413, 0.1173, 0.1455, 0.2115, 0.1303, 0.1415, 0.1127, 0.0000],\n",
      "          [0.1671, 0.1625, 0.1716, 0.0713, 0.1306, 0.0780, 0.1089, 0.1098]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "Q = 8\n",
    "K = 8\n",
    "max_len = 32\n",
    "attention = torch.rand([1, 1, Q, K])\n",
    "mask = torch.ones(1, 1, max_len, max_len, dtype=torch.bool).tril(diagonal=0)\n",
    "attention = attention.masked_fill(mask[:, :, :Q,:Q] == 0, float('-inf'))\n",
    "print(\"mask_attention:\\n\", attention)\n",
    "attention = torch.softmax(attention, dim=-1)\n",
    "print(\"softmax_attention:\\n\", attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
